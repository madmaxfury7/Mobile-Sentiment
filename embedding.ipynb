{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"embedding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYzVqAA74Vf9Nw8arcX02N"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"b-hMecoZ3ghJ"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import codecs\n","import re\n","\n","# Paths to Different Data\n","path=\"D:/data-mining-project/Data/\"\n","gloveFile = 'D:/data-mining-project/Data/glove/glove_6B_100d.txt'\n","vocab_path = 'D:/data-mining-project/Data/glove/vocab_glove.csv'\n","wordVectors_path = 'D:/data-mining-project/Data/inputs_model/wordVectors.csv'\n","sequence_len_path = 'D:/data-mining-project/Data/inputs_model/sequence_length.csv'\n","train_data_path ='D:/data-mining-project/Data/TrainingData/train.csv'\n","val_data_path ='D:/data-mining-project/Data/TrainingData/val.csv'\n","sent_matrix_path ='D:/data-mining-project/Data/inputs_model/sentence_matrix.csv'\n","sent_matrix_path_val ='D:/data-mining-project/Data/inputs_model/sentence_matrix_val.csv'\n","sent_matrix_path_test ='D:/data-mining-project/Data/inputs_model/sentence_matrix_test.csv'\n","sequence_len_val_path = 'D:/data-mining-project/Data/inputs_model/sequence_length_val.csv'\n","sequence_len_test_path = 'D:/data-mining-project/Data/inputs_model/sequence_length_test.csv'\n","test_data_path ='D:/data-mining-project/Data/TrainingData/test.csv'\n","\n","#print(path)\n","\n","def maxSeqLen(training_data):\n","\n","    total_words = 0\n","    sequence_length = []\n","    idx = 0\n","    for index, row in training_data.iterrows():\n","\n","        sentence = (row['Phrase'])\n","        sentence_words = sentence.split(' ')\n","        len_sentence_words = len(sentence_words)\n","        total_words = total_words + len_sentence_words\n","\n","        # get the length of the sequence of each training data\n","        sequence_length.append(len_sentence_words)\n","\n","        if idx == 0:\n","            max_seq_len = len_sentence_words\n","\n","\n","        if len_sentence_words > max_seq_len:\n","            max_seq_len = len_sentence_words\n","        idx = idx + 1\n","\n","    avg_words = total_words/index\n","\n","    # convert to numpy array\n","    sequence_length_np = np.asarray(sequence_length)\n","\n","    return max_seq_len, avg_words, sequence_length_np\n","\n","def tf_data_pipeline(data, word_idx, weight_matrix, max_seq_len):\n","    maxSeqLength = max_seq_len #Maximum length of sentence\n","    no_rows = len(data)\n","    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n","    # conver keys in dict to lower case\n","    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n","    idx = 0\n","\n","    for index, row in data.iterrows():\n","\n","\n","        sentence = (row['Phrase'])\n","        sentence_words = sentence.split(' ')\n","\n","        i = 0\n","        for word in sentence_words:\n","            #print(index)\n","            word_lwr = word.lower()\n","            try:\n","                #print (word_lwr)\n","                ids[idx][i] =  word_idx_lwr[word_lwr]\n","\n","            except Exception as e:\n","                #print (e)\n","                #print (word)\n","                if str(e) == word:\n","                    ids[idx][i] = 0\n","                continue\n","            i = i + 1\n","        idx = idx + 1\n","    return ids\n","\n","# Create Vocab subset GLove vectors\n","def word_vec_index(training_data, glove_model):\n","\n","    sentences = training_data['Phrase'] # get the phrases as a df series\n","    #sentences = sentences[0:100]\n","    sentences_concat = sentences.str.cat(sep=' ')\n","    sentence_words = re.findall(r'\\S+', sentences_concat)\n","    sentence_words_lwr = [x.lower() for x in sentence_words]\n","    subdict = {word: glove_model[word] for word in glove_model.keys() & sentence_words_lwr}\n","\n","    vocab_df = pd.DataFrame(subdict)\n","    vocab_df.to_csv(vocab_path)\n","    return vocab_df\n","\n","# Filtered Vocabulary from Glove document\n","def filter_glove(full_glove_path, data_dir):\n","    vocab = set()\n","    sentence_path = os.path.join(data_dir,'SOStr.txt')\n","    filtered_glove_path = os.path.join(data_dir, 'filtered_glove.txt')\n","    with codecs.open(sentence_path, encoding='utf-8') as f:\n","        for line in f:\n","            # Drop the trailing newline and strip backslashes. Split into words.\n","            vocab.update(line.strip().replace('\\\\', '').split('|'))\n","    nread = 0\n","    nwrote = 0\n","    with codecs.open(full_glove_path, encoding='utf-8') as f:\n","        with codecs.open(filtered_glove_path, 'w', encoding='utf-8') as out:\n","            for line in f:\n","                nread += 1\n","                line = line.strip()\n","                if not line: continue\n","                if line.split(u' ', 1)[0] in vocab:\n","                    out.write(line + '\\n')\n","                    nwrote += 1\n","    print('read %s lines, wrote %s' % (nread, nwrote))\n","    return vocab\n","\n","# Combine and split the data into train and test\n","def read_data(path):\n","    df_data_sentence = pd.read_table(path + 'dictionary.txt')\n","    df_data_sentence_processed = df_data_sentence['Phrase|Index'].str.split('|', expand=True)\n","    df_data_sentence_processed = df_data_sentence_processed.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n","    # read sentiment labels into df\n","    df_data_sentiment = pd.read_table(path + 'sentiment_labels.txt')\n","    df_data_sentiment_processed = df_data_sentiment['phrase ids|sentiment values'].str.split('|', expand=True)\n","    df_data_sentiment_processed = df_data_sentiment_processed.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n","    #combine data frames containing sentence and sentiment\n","    df_processed_all = df_data_sentence_processed.merge(df_data_sentiment_processed, how='inner', on='phrase_ids')\n","    return df_processed_all\n","\n","# Glove Vector\n","def loadGloveModel(gloveFile):\n","    print (\"Loading Glove Model\")\n","    f = open(gloveFile,'r',encoding='utf-8')\n","    model = {}\n","    for line in f:\n","        try:\n","            splitLine = line.split()\n","            word = splitLine[0]\n","            embedding = [float(val) for val in splitLine[1:]]\n","            model[word] = embedding\n","        except:\n","            print (word)\n","            continue\n","\n","    print (\"Done.\",len(model),\" words loaded!\")\n","    return model\n","\n","# Convert df to list\n","def word_list(vocab_df):\n","\n","    wordVectors = vocab_df.values.T.tolist()\n","    wordVectors_np = np.array(wordVectors)\n","    wordList = list(vocab_df.columns.values)\n","\n","    return wordList, wordVectors_np\n","\n","def training_data_split(all_data, spitPercent, data_dir):\n","\n","    msk = np.random.rand(len(all_data)) < spitPercent\n","    train_only = all_data[msk]\n","    test_and_dev = all_data[~msk]\n","\n","\n","    msk_test = np.random.rand(len(test_and_dev)) <0.5\n","    test_only = test_and_dev[msk_test]\n","    dev_only = test_and_dev[~msk_test]\n","\n","    dev_only.to_csv(os.path.join(data_dir, 'TrainingData/val.csv'))\n","    test_only.to_csv(os.path.join(data_dir, 'TrainingData/test.csv'))\n","    train_only.to_csv(os.path.join(data_dir, 'TrainingData/train.csv'))\n","\n","    return train_only, test_only, dev_only\n","\n","\n","# main function\n","all_data = read_data(path)\n","training_data = pd.read_csv(train_data_path, encoding='iso-8859-1')\n","\n","# to split the training, validation and test\n","train_df, test_df, dev_df = training_data_split(all_data,0.5,path)\n","\n","# Load glove vector\n","glove_model = filter_glove(gloveFile,path)\n","glove_model = loadGloveModel(gloveFile)\n","\n","# Get glove vector subset for training vocab\n","vocab_df = word_vec_index(train_df, glove_model)\n","glove_model = None\n","vocab_df = pd.read_csv(vocab_path, encoding='iso-8859-1')\n","\n","#Get Wordlist and word vec lists from the df\n","wordList, wordVectors = word_list(vocab_df)\n","wordVectors_df = pd.DataFrame(wordVectors)\n","wordVectors_df.to_csv(wordVectors_path)\n","\n","# get the index of the word vec for each sentences to be input to the tf algo\n","max_seq_len, avg_len, sequence_length = maxSeqLen(training_data)\n","sequence_length_df = pd.DataFrame(sequence_length)\n","sequence_length_df.to_csv(sequence_len_path)\n","\n","# training data input matrix\n","sentence_matrix = tf_data_pipeline(training_data, vocab_df, wordVectors, max_seq_len)\n","\n","# export the sentence matrix to a csv file for easy load for next iterations\n","sentence_matrix_df = pd.DataFrame(sentence_matrix)\n","sentence_matrix_df.to_csv(sent_matrix_path)\n","\n","# validation data set\n","val_data = pd.read_csv(val_data_path, encoding='iso-8859-1')\n","\n","# load glove model and generat vocab for validation data\n","glove_model = loadGloveModel(gloveFile)\n","vocab_df_val = word_vec_index(val_data, glove_model)\n","glove_model = None\n","wordList_val, wordVectors_val = word_list(vocab_df_val)\n","\n","# get max length for val data\n","max_seq_len_val, avg_len_val, sequence_length_val = maxSeqLen(val_data)\n","sequence_length_val_df = pd.DataFrame(sequence_length_val)\n","sequence_length_val_df.to_csv(sequence_len_val_path)\n","\n","# get the id matrix for val data\n","sentence_matrix_val = tf_data_pipeline(val_data, vocab_df_val, wordVectors_val, max_seq_len)\n","\n","# write the val dat to csv\n","sentence_matrix_df_val = pd.DataFrame(sentence_matrix_val)\n","sentence_matrix_df_val.to_csv(sent_matrix_path_val)\n","\n","# Test data set\n","test_data = pd.read_csv(test_data_path, encoding='iso-8859-1')\n","\n","# load glove model and generat vocab for test data\n","glove_model = loadGloveModel(gloveFile)\n","vocab_df_test = word_vec_index(val_data, glove_model)\n","glove_model = None\n","wordList_test, wordVectors_test = word_list(vocab_df_test)\n","\n","# get max length for test data\n","max_seq_len_test, avg_len_test, sequence_length_test = maxSeqLen(test_data)\n","sequence_length_test_df = pd.DataFrame(sequence_length_test)\n","sequence_length_test_df.to_csv(sequence_len_test_path)\n","\n","# get the id matrix for test data\n","sentence_matrix_test = tf_data_pipeline(test_data, vocab_df_test, wordVectors_test, max_seq_len_test)\n","\n","# write the test dat to csv\n","sentence_matrix_df_test= pd.DataFrame(sentence_matrix_test)\n","sentence_matrix_df_test.to_csv(sent_matrix_path_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTyQe1UGXxvr","executionInfo":{"status":"ok","timestamp":1601928485324,"user_tz":-330,"elapsed":1169,"user":{"displayName":"Sushant Ashutosh","photoUrl":"https://lh3.googleusercontent.com/-nMqyWbRR1tQ/AAAAAAAAAAI/AAAAAAAADgo/WIs2E6gOksQ/s64/photo.jpg","userId":"04651898088792925215"}},"outputId":"7faac804-6545-4b28-d551-eb936184f51e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!cd\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EGiZp-QiZkgV"},"source":[""],"execution_count":null,"outputs":[]}]}